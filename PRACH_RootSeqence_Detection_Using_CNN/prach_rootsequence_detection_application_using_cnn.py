# -*- coding: utf-8 -*-
"""PRACH_RootSequence_Detection_Application_using_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QHVufPjDfxyyCG8bwWRuijGoDEnVMXP-
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class myconv_net(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv1d(2,2,200)  ## Channels 2 as real and imaginary for inp & out, 100 is filter length being applied , 839-200+1 = 640
    self.conv2 = nn.Conv1d(2,2,100)   ## 640-100+1 = 541
    self.conv3 = nn.Conv1d(2,2,50)   ## 541-50+1 = 492
    self.conv4 = nn.Conv1d(2,2,128)   ## 492-128+1 = 365
    self.conv5 = nn.Conv1d(2,2,64)    ## 365 -64+1 = 302

    self.fc1 = nn.Linear(302,256)
    self.fc2 = nn.Linear(256,128)
  def forward(self, x):
        # x shape: [B, 839, 2]
        x = x.permute(0, 2, 1)  # â†’ [B, 2, 839]

        x = F.relu(self.conv1(x))  # â†’ [B, 2, 640]
        x = F.relu(self.conv2(x))  # â†’ [B, 2, 541]
        x = F.relu(self.conv3(x))  # â†’ [B, 2, 492]
        x = F.relu(self.conv4(x))  # â†’ [B, 2, 365]
        x = F.relu(self.conv5(x))  # â†’ [B, 2, 302]

        # ðŸ”¥ Max over channels (dim=1)
        x = torch.max(x, dim=1)[0]  # â†’ [B, 302]

        x = F.relu(self.fc1(x))     # â†’ [B, 256]
        x = self.fc2(x)             # â†’ [B, num_classes]

        return x

model = myconv_net()
print(model)

import torch.optim as optim
criteria = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset
import pandas as pd
import os
from google.colab import drive
drive.mount('/content/gdrive')
os.chdir('/content/gdrive/MyDrive/Colab Notebooks')
print("Loading Data")
labels = pd.read_csv('Labels.csv', header=None)
Training = pd.read_csv('Training_Data.csv', header=None)
# Stack real and imaginary components in the desired format
num_samples = Training.shape[0]  # Get the number of samples
num_features = Training.shape[1]  # Get the number of original features
Training = Training.applymap(lambda x: complex(str(x).replace("i","j")))

real = Training.applymap(lambda x: x.real)
imag = Training.applymap(lambda x: x.imag)

import numpy as np
X = torch.tensor(np.stack([real.to_numpy(), imag.to_numpy()],axis=-1), dtype=torch.float32)
print(X.shape)
Y = torch.tensor(labels.transpose().to_numpy(), dtype=torch.long)
print(Y.shape)
Y = np.argmax(Y, axis=1)

from sklearn.model_selection import train_test_split
## 0.8 train, 0.1 Val, 0.1 Test

X_temp, X_test = train_test_split(X, test_size=0.1, random_state=42)
X_train, X_val = train_test_split(X, test_size=0.111, random_state=42)

Y_temp, Y_test = train_test_split(Y, test_size=0.1, random_state=42)
Y_train, Y_val = train_test_split(Y, test_size=0.111, random_state=42)

from torch.utils.data import DataLoader, Dataset

class MyDataset(Dataset):
  def __init__(self,X,Y):
    self.X = X
    self.Y = Y
  def __len__(self):
    return len(self.X)
  def __getitem__(self,idx):
    return self.X[idx], self.Y[idx]

train_dataset = MyDataset(X_train,Y_train)
val_dataset = MyDataset(X_val,Y_val)
test_dataset = MyDataset(X_test,Y_test)

trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)
valloader = DataLoader(val_dataset, batch_size=64, shuffle=True)
testloader = DataLoader(test_dataset, batch_size=64, shuffle=True)
print(trainloader.dataset.X.__getitem__(0).shape)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
def train(model, train_loader, val_loader, criteria, optimizer, num_epochs=1):
    model.train()
    for epoch in range(num_epochs):
      running_loss = 0.0
      correct = 0
      total = 0
      for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        loss = criteria(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)
        _, predicted = torch.max(outputs, 1)
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

      train_loss = running_loss / total
      train_acc = correct / total

      # Validation at end of epoch
      val_acc = evaluate(model, val_loader)

      print(f"Epoch {epoch+1}: "
            f"Train Loss={train_loss:.4f}, "
            f"Train Acc={train_acc:.4f}, "
            f"Val Acc={val_acc:.4f}")

def evaluate(model, dataloader):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

    model.train()  # Switch back to training mode
    return correct / total

print(X_test.shape)
print(Y_test.shape)

train(model, trainloader, valloader, criteria, optimizer, num_epochs=50)

torch.save(model.state_dict(), "convnet_model.pt")
test_acc = evaluate(model, testloader)
print(f"Test Accuracy: {test_acc:.4f}")

